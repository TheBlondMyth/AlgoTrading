{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheBlondMyth/AlgoTrading/blob/main/NB2.1_Monitoring_training_validation_losses_and_configuration_of_optimiser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monitoring training/validation loss and configuration of optimiser\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This notebook demonstrates how training and validation losses can be monitored during training. The underlying problem is based on NB 1.2 where we generate some noisy data from a given relationship\n",
        "\\begin{align}\n",
        "y=f(x)=10x - x^2\n",
        "\\end{align}\n",
        "\n",
        "In particular, the $N$ samples of data $(x^i,y^i)$ are obtained via:\n",
        "\n",
        "1.   Create a list of $N$ feature values via $x^i:=10(i-1)/(N-1)$ for $i=1,...,N$ such that $x^i$'s are $N$ uniformly-spaced numbers in $[0,10]$.\n",
        "2.   Generate values of the label $y^i$ via\n",
        "\\begin{align}\n",
        "y^i=f(x^i)+\\epsilon^i \\qquad \\text{for }i=1,...,N\n",
        "\\end{align}\n",
        "where $\\epsilon^i\\sim N(0,\\eta^2)$ are i.i.d. Gaussian random variables with mean zero and standard deviation $\\eta$.\n",
        "\n",
        "For the purpose of demonstration in this notebook, we will just consider a small number of sample $N=100$.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "i5TIg_9k1fSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation of data and model setup\n",
        "\n",
        "The procedures are the same as NB 1.2 and hence will not be further elaborated."
      ],
      "metadata": {
        "id": "-3R79hygvP8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "N = 100\n",
        "eta = 1\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "obs_x = np.linspace(0, 10, N)                 # create an array of N elements ranging from 0 to 10\n",
        "true_y = 10*obs_x - obs_x**2\n",
        "obs_y = true_y + np.random.normal(0, eta, N)       # add Gaussian noises to the label values\n",
        "\n",
        "plt.plot(obs_x, obs_y, 'r*', label=\"Noisy observed data\")\n",
        "plt.plot(obs_x, true_y, 'b-', label=\"True model\")\n",
        "plt.xlabel('x')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "HfHq_nZc1gdy",
        "outputId": "291ef12f-3e4e-4c3c-f6ef-befd05d02a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f08b8c57910>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our candidate model is a feedforward neural network with 4 hidden layers and each layer carries 100 neurons. ReLU activations are used at all hidden layers and the output activation is an identity function."
      ],
      "metadata": {
        "id": "RFUVk-pR3LJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MyModel = tf.keras.models.Sequential()\n",
        "MyModel.add(tf.keras.layers.Dense(100, activation=\"relu\", input_shape=(1,)))\n",
        "for i in range(3):\n",
        "    MyModel.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
        "MyModel.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "MyModel.summary()                           # show the model summary\n",
        "MyModel.compile(loss=\"mean_squared_error\") # compile the model"
      ],
      "metadata": {
        "id": "iYjPXiYx3Pur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c996745c-62ef-4882-b775-057892167a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 100)               200       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,601\n",
            "Trainable params: 30,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we split all the available data into the training and validation sets. We will do a 50-50 split using the function \"train_test_split\" from the \"sklearn\" library."
      ],
      "metadata": {
        "id": "nbl90oyWGBhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(obs_x, obs_y, test_size=0.5, random_state=123) #fix the \"random_state\" to ensure same random split outcomes every time for reproducibility"
      ],
      "metadata": {
        "id": "2NZ4qaU9GhBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training of the model with display of training and validation losses\n",
        "\n",
        "The model can now be trained. If we want to compute both the training loss and validation loss over each epoch during training, this can be done within the \"fit\" function of TensorFlow. All we need to do is to supply the validation data to the \"fit\" function as well, and assign its output (an object) to a Python variable (we call this \"MyModelResult\").\n",
        "\n",
        "We can also set the mini-batch size to be used during the training. If we do not supply any value, the default value used in TensorFlow is 32.\n",
        "\n",
        "If you wish, you can also set verbose=1 which will then display all the training loss and validation loss values during the training. You can also simply drop \"verbose=1\" and the program by default will display the information.\n",
        "\n",
        "Now let's train the model for 1000 epochs. (This will take a while to run)"
      ],
      "metadata": {
        "id": "cSbCSzjU4f53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "MyModelResult = MyModel.fit(x_train, y_train, epochs=num_epochs, batch_size=64, verbose=1, validation_data = (x_test, y_test))"
      ],
      "metadata": {
        "id": "LOKHuWVj4hVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecfef5e-a989-410f-872d-633a108faf8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 377.3542 - val_loss: 285.0992\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 346.4626 - val_loss: 266.1068\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 326.7784 - val_loss: 247.7880\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 307.6996 - val_loss: 228.6147\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 287.5798 - val_loss: 206.9424\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 264.6739 - val_loss: 183.7988\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 239.8832 - val_loss: 161.3895\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 215.4675 - val_loss: 141.3094\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 192.9736 - val_loss: 124.9923\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 173.9119 - val_loss: 113.6463\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 159.6702 - val_loss: 107.0479\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 150.2089 - val_loss: 104.3115\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 144.9982 - val_loss: 103.7240\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 142.5988 - val_loss: 103.7854\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 141.4995 - val_loss: 103.9345\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 140.9084 - val_loss: 104.0591\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 140.5064 - val_loss: 103.9386\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 140.1683 - val_loss: 103.7248\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 139.8371 - val_loss: 103.4649\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 139.5029 - val_loss: 103.1874\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 139.1639 - val_loss: 102.8997\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 138.8197 - val_loss: 102.6099\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 138.4536 - val_loss: 102.2688\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 138.1084 - val_loss: 102.0279\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 137.7194 - val_loss: 101.6551\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 137.3384 - val_loss: 101.3319\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 136.9492 - val_loss: 100.9915\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 136.5483 - val_loss: 100.6533\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 136.1244 - val_loss: 100.2660\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 135.6945 - val_loss: 99.9323\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 135.2527 - val_loss: 99.5500\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 134.8548 - val_loss: 99.2742\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 134.3456 - val_loss: 98.6432\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 133.8597 - val_loss: 98.3972\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 133.3533 - val_loss: 97.5168\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 132.8394 - val_loss: 97.8103\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 132.2910 - val_loss: 96.4165\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 131.7337 - val_loss: 97.1998\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 131.1734 - val_loss: 94.9681\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 130.7436 - val_loss: 97.8020\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 130.4088 - val_loss: 93.3749\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 130.5651 - val_loss: 99.5738\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 130.4266 - val_loss: 92.4167\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 130.1677 - val_loss: 96.9298\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 128.4074 - val_loss: 91.3061\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 127.5969 - val_loss: 94.5386\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 126.5506 - val_loss: 90.2946\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 125.7983 - val_loss: 92.9284\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 124.9551 - val_loss: 89.0114\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 124.2532 - val_loss: 91.9023\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 123.4759 - val_loss: 87.4804\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 122.8823 - val_loss: 91.4620\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 122.1851 - val_loss: 85.9581\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 121.7792 - val_loss: 90.9978\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 120.9034 - val_loss: 84.5384\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 120.3604 - val_loss: 89.6675\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 119.2112 - val_loss: 82.9644\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 118.2862 - val_loss: 87.0050\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 116.7802 - val_loss: 81.3542\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 115.7701 - val_loss: 84.7414\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 114.4897 - val_loss: 79.5600\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 113.5367 - val_loss: 83.6266\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 112.6237 - val_loss: 77.6765\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 111.7038 - val_loss: 82.0104\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 110.4603 - val_loss: 75.8186\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 109.6564 - val_loss: 80.8260\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 108.4972 - val_loss: 73.8736\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 107.4564 - val_loss: 78.1265\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 105.6439 - val_loss: 71.6722\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 104.5716 - val_loss: 76.1181\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 103.1176 - val_loss: 69.5577\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 102.0748 - val_loss: 74.2733\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 100.6282 - val_loss: 67.4817\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 99.7079 - val_loss: 73.4894\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 98.7488 - val_loss: 65.5434\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 97.4873 - val_loss: 70.0446\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 95.3278 - val_loss: 63.1005\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 94.0036 - val_loss: 67.5710\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 92.3494 - val_loss: 60.8919\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 91.3412 - val_loss: 65.9541\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 89.8036 - val_loss: 58.8133\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 88.9756 - val_loss: 65.7709\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 88.2320 - val_loss: 57.2149\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 87.1977 - val_loss: 64.0535\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 85.8726 - val_loss: 55.0201\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 83.9317 - val_loss: 59.2689\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 81.5810 - val_loss: 52.5438\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 80.0578 - val_loss: 56.6562\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 78.5187 - val_loss: 50.5816\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 77.5033 - val_loss: 56.7578\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 77.0929 - val_loss: 49.6917\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 76.7984 - val_loss: 57.5262\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 76.3098 - val_loss: 48.6531\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 75.4313 - val_loss: 55.9245\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 74.1376 - val_loss: 46.4851\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 72.0603 - val_loss: 50.6940\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 69.5050 - val_loss: 44.0189\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 68.0321 - val_loss: 48.1109\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 66.5708 - val_loss: 42.4539\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 65.7330 - val_loss: 48.5719\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 65.5900 - val_loss: 42.4543\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 66.0225 - val_loss: 53.3904\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 68.1531 - val_loss: 43.0442\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 66.5672 - val_loss: 47.8525\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 63.2751 - val_loss: 39.4465\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 61.0934 - val_loss: 42.7945\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 58.8277 - val_loss: 37.3996\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 57.6427 - val_loss: 41.0741\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 56.5683 - val_loss: 36.4389\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 56.1122 - val_loss: 42.5494\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 56.5278 - val_loss: 37.7188\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 57.9235 - val_loss: 48.7555\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 60.5931 - val_loss: 39.2869\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 59.5179 - val_loss: 43.7285\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 56.0927 - val_loss: 34.9605\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 53.5022 - val_loss: 37.6182\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 50.7911 - val_loss: 32.4371\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 49.5386 - val_loss: 35.4147\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 48.3248 - val_loss: 31.4460\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 47.8635 - val_loss: 35.7878\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 47.6731 - val_loss: 32.1725\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 48.6591 - val_loss: 41.5027\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 51.5343 - val_loss: 35.7781\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 52.8398 - val_loss: 41.9973\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 51.5169 - val_loss: 32.6195\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 48.5636 - val_loss: 34.6977\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 45.1059 - val_loss: 28.7112\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 43.2749 - val_loss: 30.9084\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 41.4698 - val_loss: 27.0967\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 40.8176 - val_loss: 30.1853\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 40.1247 - val_loss: 27.3467\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 40.7657 - val_loss: 33.3378\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 42.0044 - val_loss: 30.9283\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 44.5939 - val_loss: 38.8957\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 46.3471 - val_loss: 31.2367\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 44.5580 - val_loss: 32.8267\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 40.8765 - val_loss: 25.7497\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 37.9291 - val_loss: 26.9160\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 35.4530 - val_loss: 22.9011\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 34.1310 - val_loss: 24.7221\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 33.0575 - val_loss: 22.1214\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 32.7740 - val_loss: 25.3225\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 32.9795 - val_loss: 23.7001\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 34.1590 - val_loss: 29.7392\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 36.3335 - val_loss: 27.4833\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 37.9123 - val_loss: 31.8170\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 37.9028 - val_loss: 25.6023\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 35.5099 - val_loss: 26.1826\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 32.6997 - val_loss: 20.9941\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 30.1319 - val_loss: 21.5892\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 28.2942 - val_loss: 18.7004\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 27.1793 - val_loss: 20.0866\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 26.5287 - val_loss: 18.5238\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 26.5142 - val_loss: 21.3371\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 27.2090 - val_loss: 20.7751\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 28.4547 - val_loss: 25.2619\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 30.3868 - val_loss: 24.0428\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 31.3365 - val_loss: 25.8374\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 30.7044 - val_loss: 21.6364\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 28.5377 - val_loss: 21.2280\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 26.3365 - val_loss: 17.7639\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 24.1971 - val_loss: 17.6617\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 22.8183 - val_loss: 15.9024\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 21.8675 - val_loss: 16.6260\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 21.5513 - val_loss: 16.1724\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 21.6973 - val_loss: 18.2906\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 22.7851 - val_loss: 18.7587\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 23.8672 - val_loss: 21.2076\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 25.2474 - val_loss: 20.6964\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 25.4257 - val_loss: 20.6548\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 24.5817 - val_loss: 18.5831\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 23.0361 - val_loss: 17.5918\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 21.5835 - val_loss: 15.9744\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 20.1490 - val_loss: 15.3679\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 19.3119 - val_loss: 14.8782\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 18.7145 - val_loss: 15.0430\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 18.7909 - val_loss: 15.5711\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 19.0152 - val_loss: 16.4458\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 19.9137 - val_loss: 17.5154\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 20.5427 - val_loss: 17.8558\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 21.0832 - val_loss: 18.2937\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 21.0031 - val_loss: 17.1448\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 20.2909 - val_loss: 16.8916\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 19.4257 - val_loss: 15.4398\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 18.5608 - val_loss: 15.4006\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 17.7661 - val_loss: 14.1823\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 17.2326 - val_loss: 14.8274\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 16.9635 - val_loss: 14.1393\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 17.0492 - val_loss: 15.4409\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 17.2687 - val_loss: 14.8998\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 17.6387 - val_loss: 16.4807\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 17.9880 - val_loss: 15.4100\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 18.0075 - val_loss: 16.7438\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 18.0268 - val_loss: 14.9667\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 17.4824 - val_loss: 16.0658\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 17.2222 - val_loss: 14.0580\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 16.5122 - val_loss: 15.4197\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 16.4328 - val_loss: 13.6528\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 16.0172 - val_loss: 15.2927\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 16.1204 - val_loss: 13.6768\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 15.9351 - val_loss: 15.5912\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 16.2076 - val_loss: 13.9187\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 16.0696 - val_loss: 15.8911\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 16.3245 - val_loss: 13.8271\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 15.8910 - val_loss: 15.7619\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 16.0684 - val_loss: 13.5065\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 15.4999 - val_loss: 15.4643\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 15.6731 - val_loss: 13.1095\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 15.0503 - val_loss: 15.0677\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 15.1753 - val_loss: 12.9784\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 14.8305 - val_loss: 15.3265\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 15.2731 - val_loss: 13.0657\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 14.8500 - val_loss: 15.3847\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 15.1994 - val_loss: 13.0220\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 14.7210 - val_loss: 15.4377\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 15.1268 - val_loss: 12.9667\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 14.5948 - val_loss: 15.5747\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 15.1553 - val_loss: 12.8069\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 14.3897 - val_loss: 15.2293\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 14.7659 - val_loss: 12.4290\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 13.9548 - val_loss: 14.9804\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 14.4509 - val_loss: 12.2804\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 13.7385 - val_loss: 15.1750\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 14.5416 - val_loss: 12.2879\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 13.7169 - val_loss: 14.7665\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 14.0690 - val_loss: 12.2659\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 13.5963 - val_loss: 15.4635\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 14.6074 - val_loss: 12.4923\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 13.7813 - val_loss: 15.0759\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 14.1948 - val_loss: 12.1476\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 13.3757 - val_loss: 15.0968\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 14.1513 - val_loss: 11.9854\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 13.1798 - val_loss: 14.7325\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 13.7417 - val_loss: 11.8923\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 13.0106 - val_loss: 15.1840\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 14.0838 - val_loss: 11.9844\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 13.0975 - val_loss: 14.7715\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 13.6312 - val_loss: 11.8724\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 12.8922 - val_loss: 15.2775\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 14.0381 - val_loss: 11.8408\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 12.8785 - val_loss: 14.5408\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 13.3017 - val_loss: 11.6701\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 12.5985 - val_loss: 15.1697\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 13.8074 - val_loss: 11.7773\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 12.7171 - val_loss: 14.6065\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 13.2521 - val_loss: 11.5962\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 12.4436 - val_loss: 14.9184\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 13.4624 - val_loss: 11.6344\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 12.4602 - val_loss: 14.7908\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 13.3036 - val_loss: 11.5482\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 12.3384 - val_loss: 14.8177\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 13.2886 - val_loss: 11.4692\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 12.2327 - val_loss: 14.5530\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 12.9928 - val_loss: 11.4235\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 12.1263 - val_loss: 15.0296\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 13.3869 - val_loss: 11.4072\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 12.1503 - val_loss: 14.2777\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 12.6765 - val_loss: 11.1575\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 11.8093 - val_loss: 14.7219\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 13.0096 - val_loss: 11.2107\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 11.8729 - val_loss: 14.2706\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 12.5672 - val_loss: 11.1417\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 11.7093 - val_loss: 15.1153\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 13.2616 - val_loss: 11.2396\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 11.8744 - val_loss: 14.2245\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 12.4600 - val_loss: 10.9320\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 11.4507 - val_loss: 14.8112\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 12.9228 - val_loss: 11.0138\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 11.5956 - val_loss: 14.0774\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 12.2618 - val_loss: 10.7118\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 11.1807 - val_loss: 14.5581\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 12.6346 - val_loss: 10.6633\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 11.1906 - val_loss: 13.8729\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 11.9906 - val_loss: 10.6282\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 11.0545 - val_loss: 14.5535\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 12.5345 - val_loss: 10.7486\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 11.2535 - val_loss: 14.1112\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 12.1049 - val_loss: 10.7866\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 11.1422 - val_loss: 14.9347\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 12.7833 - val_loss: 10.8285\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 11.2747 - val_loss: 13.9685\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 11.9370 - val_loss: 10.4476\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 10.7762 - val_loss: 14.4571\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 12.3142 - val_loss: 10.4443\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.8223 - val_loss: 13.6779\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 11.6171 - val_loss: 10.2343\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 10.5089 - val_loss: 14.8020\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 12.5434 - val_loss: 10.3994\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 10.7965 - val_loss: 13.6289\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 11.5480 - val_loss: 9.9119\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 10.1690 - val_loss: 13.9775\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 11.7814 - val_loss: 10.1037\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.3634 - val_loss: 14.3365\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 12.0430 - val_loss: 10.6992\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 10.8801 - val_loss: 14.7615\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 12.4381 - val_loss: 10.1248\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 10.4007 - val_loss: 13.6647\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 11.4952 - val_loss: 9.5883\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9.8132 - val_loss: 13.1775\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 11.0712 - val_loss: 9.5839\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9.7919 - val_loss: 13.5241\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 11.2958 - val_loss: 10.0803\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 10.1994 - val_loss: 14.3968\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 12.0236 - val_loss: 10.2187\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 10.4144 - val_loss: 13.6359\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 11.3856 - val_loss: 10.0403\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 10.1161 - val_loss: 14.2739\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 11.9144 - val_loss: 9.9147\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 10.0981 - val_loss: 13.0778\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.8934 - val_loss: 9.6647\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 9.7385 - val_loss: 13.5910\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 11.2788 - val_loss: 9.9592\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 10.0130 - val_loss: 13.5856\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 11.2535 - val_loss: 10.1697\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 10.1513 - val_loss: 14.2683\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 11.8138 - val_loss: 10.1342\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 10.1911 - val_loss: 13.5405\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 11.2096 - val_loss: 9.6470\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9.7157 - val_loss: 13.3140\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 10.9846 - val_loss: 9.3842\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9.4984 - val_loss: 12.5240\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 10.2920 - val_loss: 9.4119\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 9.3700 - val_loss: 13.7193\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 11.2546 - val_loss: 10.1717\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 10.0937 - val_loss: 14.0242\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 11.5075 - val_loss: 10.0304\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9.9530 - val_loss: 13.4328\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 10.9996 - val_loss: 9.7482\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9.6600 - val_loss: 13.5807\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 11.1084 - val_loss: 9.4714\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9.4689 - val_loss: 12.5355\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 10.2092 - val_loss: 9.2432\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9.1422 - val_loss: 13.3135\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 10.8107 - val_loss: 9.6701\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9.6177 - val_loss: 13.4383\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 10.9211 - val_loss: 9.7377\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9.6327 - val_loss: 13.5219\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 10.9568 - val_loss: 9.6811\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9.5456 - val_loss: 13.0238\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 10.5441 - val_loss: 9.5647\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 9.3794 - val_loss: 13.4428\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 10.8698 - val_loss: 9.5841\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9.4375 - val_loss: 12.7997\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 10.3196 - val_loss: 9.4884\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9.2549 - val_loss: 13.3073\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.7292 - val_loss: 9.5724\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9.3763 - val_loss: 12.8877\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 10.3580 - val_loss: 9.4497\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9.1961 - val_loss: 13.1614\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 10.5792 - val_loss: 9.4922\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9.2710 - val_loss: 12.7253\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.2118 - val_loss: 9.2002\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.9710 - val_loss: 12.7180\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 10.1613 - val_loss: 9.3439\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9.0500 - val_loss: 13.1495\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 10.5028 - val_loss: 9.4988\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 9.2286 - val_loss: 12.7244\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 10.1600 - val_loss: 9.2914\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8.9499 - val_loss: 12.7403\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.1557 - val_loss: 9.1352\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 8.8436 - val_loss: 12.4536\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9.8789 - val_loss: 9.1323\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8.7760 - val_loss: 12.7507\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 10.1080 - val_loss: 9.2502\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.8803 - val_loss: 12.7152\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.0566 - val_loss: 9.2690\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 8.8589 - val_loss: 12.6990\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 10.0609 - val_loss: 9.1023\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 8.7447 - val_loss: 12.3541\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 9.7379 - val_loss: 9.0941\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8.6442 - val_loss: 12.9836\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 10.2019 - val_loss: 9.3081\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8.9098 - val_loss: 12.7132\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 10.0048 - val_loss: 9.0051\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8.5837 - val_loss: 12.1756\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 9.5639 - val_loss: 8.8433\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.3939 - val_loss: 12.3276\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9.6002 - val_loss: 8.9665\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 8.4057 - val_loss: 13.4785\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 10.5074 - val_loss: 9.9719\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 9.3637 - val_loss: 13.8165\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 10.8232 - val_loss: 9.3876\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 8.8859 - val_loss: 12.2976\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 9.5873 - val_loss: 8.5575\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 8.0530 - val_loss: 11.8964\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 9.2066 - val_loss: 8.5049\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.9609 - val_loss: 12.1462\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9.3690 - val_loss: 8.8726\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 8.2755 - val_loss: 12.6161\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9.7429 - val_loss: 9.1718\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 8.5172 - val_loss: 13.0267\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 10.0986 - val_loss: 9.1069\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 8.4852 - val_loss: 12.4660\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9.6475 - val_loss: 8.6348\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 8.0478 - val_loss: 11.9088\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 9.1479 - val_loss: 8.4725\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 7.8693 - val_loss: 11.7494\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.9985 - val_loss: 8.7003\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.0120 - val_loss: 12.8481\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9.8432 - val_loss: 9.2174\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.5167 - val_loss: 12.9045\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 9.8950 - val_loss: 9.1610\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 8.4479 - val_loss: 12.6478\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9.7313 - val_loss: 8.6354\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8.0008 - val_loss: 11.7068\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.9591 - val_loss: 8.2775\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.6254 - val_loss: 11.6808\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8.8399 - val_loss: 8.5684\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.8208 - val_loss: 12.4258\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 9.4105 - val_loss: 9.1159\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8.3124 - val_loss: 13.2015\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 10.0805 - val_loss: 9.1821\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8.4252 - val_loss: 12.4417\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9.4963 - val_loss: 8.4222\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 7.7333 - val_loss: 11.5958\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 8.7732 - val_loss: 8.2247\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.4855 - val_loss: 11.8094\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.8804 - val_loss: 8.5436\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.7448 - val_loss: 12.4399\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9.3701 - val_loss: 8.8466\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.0363 - val_loss: 12.6720\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 9.5700 - val_loss: 8.7382\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7.9409 - val_loss: 12.2603\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 9.2455 - val_loss: 8.4853\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 7.6998 - val_loss: 12.0832\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9.0712 - val_loss: 8.4537\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.6515 - val_loss: 12.1729\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 9.1105 - val_loss: 8.4968\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.6705 - val_loss: 12.3648\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9.2544 - val_loss: 8.4633\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 7.6862 - val_loss: 12.0895\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9.0471 - val_loss: 8.3297\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 7.5131 - val_loss: 12.0063\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8.9437 - val_loss: 8.3075\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.4618 - val_loss: 12.3543\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9.2067 - val_loss: 8.3853\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.5507 - val_loss: 12.0781\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 8.9514 - val_loss: 8.2495\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 7.3835 - val_loss: 12.1459\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 8.9417 - val_loss: 8.2453\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 7.3913 - val_loss: 12.0349\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8.8885 - val_loss: 8.1843\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7.3099 - val_loss: 12.3655\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9.1129 - val_loss: 8.2970\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 7.4018 - val_loss: 12.3520\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9.1380 - val_loss: 8.0208\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.1960 - val_loss: 11.7738\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.6423 - val_loss: 7.8674\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.0000 - val_loss: 12.0277\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8.8008 - val_loss: 8.0300\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.1393 - val_loss: 12.0648\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.8112 - val_loss: 8.0056\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 7.0998 - val_loss: 12.3328\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.9962 - val_loss: 8.1058\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 7.1990 - val_loss: 12.1273\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.8537 - val_loss: 7.8114\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.8872 - val_loss: 11.8958\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 8.6717 - val_loss: 7.7112\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.7838 - val_loss: 11.7190\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.5286 - val_loss: 7.6734\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6.7287 - val_loss: 11.9525\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8.7040 - val_loss: 7.8560\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6.8722 - val_loss: 12.2520\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 8.8990 - val_loss: 7.8844\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 6.9088 - val_loss: 12.0586\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 8.7757 - val_loss: 7.6919\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.7342 - val_loss: 11.3707\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8.2582 - val_loss: 7.6158\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.6268 - val_loss: 11.8497\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 8.5418 - val_loss: 7.9460\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.8953 - val_loss: 12.6105\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 9.1155 - val_loss: 8.1867\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 7.1498 - val_loss: 12.3993\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.9706 - val_loss: 7.8702\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 6.8912 - val_loss: 11.5011\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8.3411 - val_loss: 7.5375\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 6.5604 - val_loss: 11.4179\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 8.2764 - val_loss: 7.6031\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.6032 - val_loss: 11.5018\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.2618 - val_loss: 7.9033\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 6.7919 - val_loss: 12.5406\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 9.0064 - val_loss: 8.3280\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 7.1994 - val_loss: 12.7655\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9.1753 - val_loss: 8.1880\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.0881 - val_loss: 11.9559\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.6253 - val_loss: 7.5089\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 6.5037 - val_loss: 10.8564\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 7.8011 - val_loss: 7.0049\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 6.0405 - val_loss: 10.6135\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 7.5750 - val_loss: 7.5156\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.4097 - val_loss: 12.1513\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8.6520 - val_loss: 8.4148\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.1979 - val_loss: 13.2355\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 9.5396 - val_loss: 8.4596\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.2977 - val_loss: 12.3100\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 8.8776 - val_loss: 7.5930\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 6.5668 - val_loss: 10.8744\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.8053 - val_loss: 6.8684\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.9195 - val_loss: 10.3501\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.3359 - val_loss: 7.1169\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.0635 - val_loss: 11.2252\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 7.9470 - val_loss: 8.0753\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 6.8577 - val_loss: 12.8115\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 9.1808 - val_loss: 8.6117\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 7.3788 - val_loss: 13.1596\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 9.4687 - val_loss: 8.0349\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.9356 - val_loss: 11.3640\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8.1421 - val_loss: 6.9471\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5.9800 - val_loss: 9.8879\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.0113 - val_loss: 6.6764\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.6542 - val_loss: 10.4611\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.3509 - val_loss: 7.5977\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6.4035 - val_loss: 12.5004\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 8.8929 - val_loss: 8.5061\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.2674 - val_loss: 13.1093\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9.4051 - val_loss: 8.1917\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.0302 - val_loss: 11.4098\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.1793 - val_loss: 7.0392\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.0164 - val_loss: 10.1073\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.1193 - val_loss: 6.7343\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.6801 - val_loss: 10.4768\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7.3387 - val_loss: 7.3710\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6.2123 - val_loss: 11.8571\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.4018 - val_loss: 8.2885\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.0119 - val_loss: 12.6691\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9.0710 - val_loss: 8.2291\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 7.0056 - val_loss: 11.8080\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 8.4593 - val_loss: 7.3342\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 6.2473 - val_loss: 10.4128\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.3747 - val_loss: 6.8386\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.7653 - val_loss: 10.3126\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.2293 - val_loss: 7.2228\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 6.0395 - val_loss: 11.2815\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.9491 - val_loss: 7.8684\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.6165 - val_loss: 12.1723\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 8.6562 - val_loss: 8.2653\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.9616 - val_loss: 12.0935\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.6590 - val_loss: 7.6735\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 6.4908 - val_loss: 10.8979\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.6979 - val_loss: 7.0007\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.8643 - val_loss: 10.1880\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.1408 - val_loss: 6.8285\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.6927 - val_loss: 10.3402\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 7.2011 - val_loss: 7.2801\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6.0410 - val_loss: 11.6647\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8.1822 - val_loss: 8.2469\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6.8859 - val_loss: 12.4633\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 8.8992 - val_loss: 8.0505\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.7995 - val_loss: 11.3400\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 8.0906 - val_loss: 7.0129\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.9285 - val_loss: 9.8540\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6.8601 - val_loss: 6.7672\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 5.5738 - val_loss: 10.3716\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.2742 - val_loss: 7.2612\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.0249 - val_loss: 11.3183\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 7.9050 - val_loss: 8.0293\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.6669 - val_loss: 12.1910\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 8.6463 - val_loss: 7.9762\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6.6744 - val_loss: 11.4388\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.0832 - val_loss: 7.0534\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.9201 - val_loss: 9.9858\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.9839 - val_loss: 6.5914\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 5.4649 - val_loss: 9.4972\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6.6085 - val_loss: 6.9199\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.6746 - val_loss: 11.0398\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.6547 - val_loss: 7.8851\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.5083 - val_loss: 12.3623\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 8.7233 - val_loss: 8.4466\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.0308 - val_loss: 12.4724\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8.8283 - val_loss: 7.7014\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.4340 - val_loss: 10.6676\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 7.5157 - val_loss: 6.7701\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.6398 - val_loss: 9.1773\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.3554 - val_loss: 6.5288\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.3009 - val_loss: 10.0434\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 6.9218 - val_loss: 7.3514\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.9758 - val_loss: 11.9935\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 8.3936 - val_loss: 8.4679\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6.9875 - val_loss: 12.5705\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.9162 - val_loss: 7.8788\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.5468 - val_loss: 10.8019\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.5969 - val_loss: 6.6005\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 5.4632 - val_loss: 9.2269\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.3832 - val_loss: 6.2847\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.1180 - val_loss: 9.1674\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.2667 - val_loss: 6.9069\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.5657 - val_loss: 11.2567\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7.7714 - val_loss: 8.2702\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.7552 - val_loss: 12.6804\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 8.9244 - val_loss: 8.3912\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6.9445 - val_loss: 11.9282\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 8.4096 - val_loss: 7.0922\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.8778 - val_loss: 9.5008\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 6.6111 - val_loss: 6.1367\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.0034 - val_loss: 8.4517\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.7797 - val_loss: 6.2472\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.9947 - val_loss: 9.9055\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 6.7286 - val_loss: 7.6431\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.1457 - val_loss: 12.7724\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 8.9187 - val_loss: 8.9813\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.3656 - val_loss: 13.4302\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 9.5034 - val_loss: 7.8416\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.5312 - val_loss: 10.0211\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.0316 - val_loss: 5.9792\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.8986 - val_loss: 7.7798\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.3100 - val_loss: 5.6345\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.4643 - val_loss: 8.4008\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5.6475 - val_loss: 6.7555\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.3497 - val_loss: 11.5763\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.9373 - val_loss: 8.7640\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.0814 - val_loss: 14.3192\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 10.1043 - val_loss: 8.8929\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.3380 - val_loss: 11.9833\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 8.4362 - val_loss: 6.6359\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.4753 - val_loss: 8.3608\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.7583 - val_loss: 5.4592\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 4.3732 - val_loss: 7.4958\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.0215 - val_loss: 5.7679\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 4.5139 - val_loss: 9.2454\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.2123 - val_loss: 7.7514\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.1461 - val_loss: 13.3199\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 9.2914 - val_loss: 9.4484\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 7.7234 - val_loss: 14.2563\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 10.0788 - val_loss: 7.8187\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 6.4656 - val_loss: 9.6563\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.7522 - val_loss: 5.5671\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4.5322 - val_loss: 6.8596\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4.6253 - val_loss: 5.0892\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 3.9557 - val_loss: 7.5929\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.0167 - val_loss: 6.4697\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.0263 - val_loss: 11.6115\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 7.9178 - val_loss: 8.9028\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 7.1478 - val_loss: 14.6731\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 10.3560 - val_loss: 9.0228\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.3961 - val_loss: 12.3911\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.7172 - val_loss: 6.5369\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.3910 - val_loss: 7.8862\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.3963 - val_loss: 5.1892\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.1044 - val_loss: 6.9263\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.5968 - val_loss: 5.4880\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.2336 - val_loss: 8.6752\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.7895 - val_loss: 7.5273\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.8882 - val_loss: 13.1834\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 9.1108 - val_loss: 9.7805\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.8946 - val_loss: 15.3802\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 10.9348 - val_loss: 7.8576\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.5377 - val_loss: 9.1412\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6.3672 - val_loss: 5.2157\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.2118 - val_loss: 6.4679\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.3153 - val_loss: 4.8123\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 3.6910 - val_loss: 7.0036\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.5876 - val_loss: 5.9985\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.5977 - val_loss: 10.3409\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.9838 - val_loss: 8.9895\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.1248 - val_loss: 16.0625\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 11.3669 - val_loss: 9.3103\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.6575 - val_loss: 12.6302\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 8.8319 - val_loss: 6.2466\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 5.1136 - val_loss: 7.1537\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.8801 - val_loss: 4.7372\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 3.6963 - val_loss: 6.2552\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.0851 - val_loss: 5.1201\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3.8639 - val_loss: 8.1087\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.3521 - val_loss: 7.3561\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.6781 - val_loss: 13.3851\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 9.2258 - val_loss: 9.6683\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 7.7639 - val_loss: 15.1140\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 10.7080 - val_loss: 8.0981\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 6.7019 - val_loss: 9.9205\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.8595 - val_loss: 5.4468\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.3626 - val_loss: 6.6738\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.4521 - val_loss: 4.8577\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3.6994 - val_loss: 6.8918\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.5137 - val_loss: 5.7716\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 4.3753 - val_loss: 9.6354\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.4457 - val_loss: 8.2259\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.4116 - val_loss: 14.8153\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 10.3250 - val_loss: 9.2676\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 7.5157 - val_loss: 13.0883\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 9.1313 - val_loss: 6.6794\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.4243 - val_loss: 8.1781\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 5.5752 - val_loss: 5.1906\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4.0474 - val_loss: 6.6062\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.3652 - val_loss: 5.1029\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 3.8537 - val_loss: 7.6085\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.0104 - val_loss: 6.6400\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.0590 - val_loss: 11.7498\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.9868 - val_loss: 8.6722\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.8519 - val_loss: 14.1156\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 9.8252 - val_loss: 8.3902\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6.7624 - val_loss: 11.0351\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.6231 - val_loss: 6.0060\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.7940 - val_loss: 7.4633\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.0190 - val_loss: 5.1849\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 3.9723 - val_loss: 7.0245\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4.6301 - val_loss: 5.6632\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.2669 - val_loss: 9.0353\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 6.0079 - val_loss: 7.6574\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.8971 - val_loss: 13.1121\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 9.0022 - val_loss: 8.4023\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.6840 - val_loss: 12.4698\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 8.6056 - val_loss: 6.8036\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5.4526 - val_loss: 8.7425\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 5.9297 - val_loss: 5.7093\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.4388 - val_loss: 7.8500\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.1936 - val_loss: 5.7405\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.3827 - val_loss: 8.7389\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.7598 - val_loss: 6.8744\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 5.2765 - val_loss: 11.1123\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.5122 - val_loss: 7.8121\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.1308 - val_loss: 11.8621\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 8.1251 - val_loss: 7.0919\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.6417 - val_loss: 9.6900\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 6.5790 - val_loss: 6.2146\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.8740 - val_loss: 8.5936\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5.7122 - val_loss: 6.0821\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.6675 - val_loss: 9.0434\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.9871 - val_loss: 6.7144\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.1612 - val_loss: 10.3874\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 6.9793 - val_loss: 7.4725\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 5.8021 - val_loss: 11.0971\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 7.5279 - val_loss: 7.0404\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.5382 - val_loss: 9.9944\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 6.7524 - val_loss: 6.3404\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.9468 - val_loss: 8.5842\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.7780 - val_loss: 6.0476\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.6442 - val_loss: 8.8855\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 5.8851 - val_loss: 6.5262\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.9862 - val_loss: 9.9703\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 6.6242 - val_loss: 7.3137\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5.6294 - val_loss: 11.5939\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 7.8224 - val_loss: 7.3327\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 5.7487 - val_loss: 10.3174\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 6.9816 - val_loss: 6.4092\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 4.9960 - val_loss: 8.5811\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.7483 - val_loss: 5.8811\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 4.4939 - val_loss: 8.2968\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.5202 - val_loss: 6.1778\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.6886 - val_loss: 9.7085\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.4323 - val_loss: 7.1578\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 5.4956 - val_loss: 11.0974\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.4691 - val_loss: 7.3281\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.7172 - val_loss: 10.6576\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 7.2014 - val_loss: 6.6813\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.2049 - val_loss: 9.1582\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.1342 - val_loss: 6.0465\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.6351 - val_loss: 8.5094\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.6174 - val_loss: 6.0647\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.5930 - val_loss: 9.1210\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6.0171 - val_loss: 6.6988\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.1017 - val_loss: 10.4118\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.9509 - val_loss: 7.1714\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.5489 - val_loss: 10.7184\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.2331 - val_loss: 6.8979\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.3536 - val_loss: 9.4768\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.3350 - val_loss: 6.2350\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.7702 - val_loss: 8.5399\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5.6716 - val_loss: 6.0971\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.6124 - val_loss: 9.0690\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 5.9877 - val_loss: 6.6085\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.0103 - val_loss: 10.1000\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.6942 - val_loss: 7.1507\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.4763 - val_loss: 10.9897\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.3771 - val_loss: 7.1995\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5.5702 - val_loss: 10.1576\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.8009 - val_loss: 6.3754\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.9023 - val_loss: 8.5442\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.6783 - val_loss: 5.8320\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.4193 - val_loss: 8.0783\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.3630 - val_loss: 5.9995\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.5197 - val_loss: 9.1848\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.0530 - val_loss: 6.8377\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5.1788 - val_loss: 10.6658\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.0987 - val_loss: 7.5009\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.7639 - val_loss: 11.1789\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.5256 - val_loss: 6.9141\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.3592 - val_loss: 9.2249\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.1525 - val_loss: 5.8072\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4.4364 - val_loss: 7.8020\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.1515 - val_loss: 5.6746\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4.2528 - val_loss: 8.3781\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.4864 - val_loss: 6.3975\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.8002 - val_loss: 10.0729\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6.6613 - val_loss: 7.3001\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 5.5671 - val_loss: 11.2127\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 7.5507 - val_loss: 7.3706\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.6972 - val_loss: 10.4742\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.0430 - val_loss: 6.3181\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.8569 - val_loss: 8.7592\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.7858 - val_loss: 5.9217\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 4.4820 - val_loss: 8.4147\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.5086 - val_loss: 5.9687\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.4967 - val_loss: 8.7752\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.7651 - val_loss: 6.4286\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.8508 - val_loss: 9.7770\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.4663 - val_loss: 6.9507\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.2935 - val_loss: 10.6336\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.0723 - val_loss: 7.1505\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.4792 - val_loss: 10.3997\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 6.9204 - val_loss: 6.4195\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.9452 - val_loss: 8.4772\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.5934 - val_loss: 5.6643\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.2835 - val_loss: 7.8438\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.1596 - val_loss: 5.8095\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.3467 - val_loss: 8.7353\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.7246 - val_loss: 6.6570\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.0010 - val_loss: 10.4331\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.9381 - val_loss: 7.3174\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.5994 - val_loss: 11.0038\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.3770 - val_loss: 6.9136\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.3369 - val_loss: 9.5590\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 6.3370 - val_loss: 5.8910\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.4949 - val_loss: 7.7836\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 5.1337 - val_loss: 5.4152\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.0630 - val_loss: 7.6940\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.0174 - val_loss: 5.9128\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.4104 - val_loss: 9.2682\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.0622 - val_loss: 6.9411\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 5.2458 - val_loss: 10.9460\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.2843 - val_loss: 7.5622\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.7953 - val_loss: 11.1312\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.4839 - val_loss: 6.6194\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.1277 - val_loss: 8.4470\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5.5962 - val_loss: 5.4141\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.0926 - val_loss: 7.2010\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.6916 - val_loss: 5.3488\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 3.9546 - val_loss: 7.8974\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.1365 - val_loss: 6.3032\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.6915 - val_loss: 10.0277\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6.6268 - val_loss: 7.3495\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.5848 - val_loss: 11.3679\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.6143 - val_loss: 7.2041\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.5478 - val_loss: 10.0580\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.6906 - val_loss: 6.0394\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.6072 - val_loss: 7.7964\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.1323 - val_loss: 5.2774\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 3.9334 - val_loss: 7.2295\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.7058 - val_loss: 5.6058\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.1242 - val_loss: 8.6363\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 5.6222 - val_loss: 6.7261\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.0220 - val_loss: 10.6392\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 7.0652 - val_loss: 7.3472\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.5965 - val_loss: 11.1004\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 7.4321 - val_loss: 6.7188\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 5.1705 - val_loss: 8.8816\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5.8876 - val_loss: 5.6007\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 4.2206 - val_loss: 7.4544\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.8663 - val_loss: 5.2979\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3.9235 - val_loss: 7.5455\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.8940 - val_loss: 5.8953\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.3509 - val_loss: 9.0870\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.9569 - val_loss: 6.9745\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.2272 - val_loss: 10.9375\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.3040 - val_loss: 7.4069\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.6532 - val_loss: 10.9609\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.2952 - val_loss: 6.4329\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.9188 - val_loss: 8.2914\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5.4717 - val_loss: 5.3189\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3.9756 - val_loss: 7.0981\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.6112 - val_loss: 5.2600\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 3.8581 - val_loss: 7.6885\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.9944 - val_loss: 6.0632\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.4783 - val_loss: 9.4737\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.2346 - val_loss: 7.1866\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 5.3911 - val_loss: 11.2361\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 7.4825 - val_loss: 7.1272\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.4406 - val_loss: 10.1099\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.7631 - val_loss: 5.9597\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.5437 - val_loss: 7.5924\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 4.9818 - val_loss: 5.2164\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3.8491 - val_loss: 7.1179\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.5988 - val_loss: 5.4712\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 3.9881 - val_loss: 8.2976\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.3904 - val_loss: 6.6294\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 4.9002 - val_loss: 10.5464\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 6.9707 - val_loss: 7.4018\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.6004 - val_loss: 11.3391\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 7.5670 - val_loss: 6.6727\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.0729 - val_loss: 8.6258\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.7160 - val_loss: 5.4906\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 4.0969 - val_loss: 7.1089\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.6550 - val_loss: 5.1346\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3.7458 - val_loss: 7.1184\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.5883 - val_loss: 5.6767\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.1194 - val_loss: 8.9094\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.7972 - val_loss: 7.0730\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.2465 - val_loss: 11.2519\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 7.5180 - val_loss: 7.1585\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 5.4418 - val_loss: 9.9999\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.6810 - val_loss: 6.0631\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.5877 - val_loss: 7.8690\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.1538 - val_loss: 5.4073\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3.9708 - val_loss: 7.4113\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.7963 - val_loss: 5.5614\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.0477 - val_loss: 8.3473\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.4129 - val_loss: 6.3905\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.7118 - val_loss: 9.7651\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6.4356 - val_loss: 6.9020\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.1683 - val_loss: 10.0530\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.6686 - val_loss: 6.4213\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.8347 - val_loss: 8.5719\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.6446 - val_loss: 5.7260\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4.2358 - val_loss: 7.9327\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 5.1614 - val_loss: 5.6783\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.1673 - val_loss: 8.1909\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 5.3392 - val_loss: 6.0101\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4.4348 - val_loss: 8.8949\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.8301 - val_loss: 6.4225\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.7575 - val_loss: 9.3998\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 6.1764 - val_loss: 6.5013\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.8389 - val_loss: 9.3668\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.1140 - val_loss: 6.2126\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 4.6099 - val_loss: 8.5477\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.5795 - val_loss: 5.8032\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.2863 - val_loss: 8.0926\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5.2927 - val_loss: 5.8318\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 4.3101 - val_loss: 8.3464\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 5.4800 - val_loss: 6.0683\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.4710 - val_loss: 8.8664\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.8084 - val_loss: 6.3595\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.7018 - val_loss: 9.2902\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 6.0830 - val_loss: 6.3745\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.7139 - val_loss: 9.1943\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.9439 - val_loss: 6.1898\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.5726 - val_loss: 8.6442\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 5.6413 - val_loss: 5.8136\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.2918 - val_loss: 8.1012\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.3063 - val_loss: 5.6834\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.1781 - val_loss: 8.1115\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5.3044 - val_loss: 5.8770\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.3133 - val_loss: 8.6090\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 5.6143 - val_loss: 6.1983\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.5731 - val_loss: 9.1167\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.9728 - val_loss: 6.4013\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 4.7391 - val_loss: 9.5481\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.2091 - val_loss: 6.3250\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.6942 - val_loss: 8.8523\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.7720 - val_loss: 5.8663\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.3367 - val_loss: 8.0860\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.2673 - val_loss: 5.5436\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.0710 - val_loss: 7.8288\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.1042 - val_loss: 5.6376\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.1267 - val_loss: 8.2346\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 5.3600 - val_loss: 6.0577\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.4470 - val_loss: 9.0491\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.9057 - val_loss: 6.4425\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 4.7631 - val_loss: 9.5317\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.1948 - val_loss: 6.4189\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4.7501 - val_loss: 9.1007\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.9458 - val_loss: 5.9662\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.4045 - val_loss: 8.1962\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.3355 - val_loss: 5.5103\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.0361 - val_loss: 7.6903\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.9995 - val_loss: 5.5008\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 4.0124 - val_loss: 7.9719\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 5.1773 - val_loss: 5.8751\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 4.2979 - val_loss: 8.7292\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.6850 - val_loss: 6.3243\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4.6577 - val_loss: 9.4131\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.1703 - val_loss: 6.4243\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.7613 - val_loss: 9.2825\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.0561 - val_loss: 6.0263\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.4631 - val_loss: 8.1354\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.3068 - val_loss: 5.4494\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3.9996 - val_loss: 7.5416\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.8920 - val_loss: 5.3931\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3.9144 - val_loss: 7.8939\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5.1015 - val_loss: 5.8257\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.2465 - val_loss: 8.6787\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 5.6514 - val_loss: 6.2947\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.6283 - val_loss: 9.4150\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 6.1641 - val_loss: 6.4015\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 4.7286 - val_loss: 9.3183\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 6.0193 - val_loss: 6.0032\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.4097 - val_loss: 8.2917\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 5.3693 - val_loss: 5.4005\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 3.9435 - val_loss: 7.4296\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 4.8222 - val_loss: 5.2504\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 3.8041 - val_loss: 7.5438\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 4.8720 - val_loss: 5.6439\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 4.0749 - val_loss: 8.4611\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 5.4728 - val_loss: 6.2306\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.5435 - val_loss: 9.4477\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 6.1752 - val_loss: 6.4973\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.8011 - val_loss: 9.6361\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 6.2289 - val_loss: 6.1675\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.5351 - val_loss: 8.2849\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.3640 - val_loss: 5.2694\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 3.8263 - val_loss: 7.3959\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.8158 - val_loss: 5.4799\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 3.9606 - val_loss: 7.9568\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.1506 - val_loss: 5.9139\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 4.2784 - val_loss: 8.8834\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.7691 - val_loss: 6.3003\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 4.5967 - val_loss: 9.5245\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 6.1683 - val_loss: 6.2987\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.6234 - val_loss: 8.8979\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.7690 - val_loss: 5.7542\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.2044 - val_loss: 7.8541\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.0756 - val_loss: 5.3302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chunk of numbers displayed above refer to the training and validation losses obtained as we perform the \"gradient descent style\" iterations. Ideally, both numbers should be decreasing as more epoches are completed such that our algorithm is indeed trying to minimise the average losses.\n",
        "\n",
        "We can now look inside the Python object \"MyModelResult\" and recover the training and validation loss values in each epoch. These are contained in a member of the object called \"history\". The training and validation losses are stored under the column named 'loss' and 'val_loss' respectively."
      ],
      "metadata": {
        "id": "1nklUg3cIlOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_train_loss = MyModelResult.history['loss']\n",
        "my_val_loss   = MyModelResult.history['val_loss']\n",
        "\n",
        "\n",
        "# The segment of code below is for plotting the results only\n",
        "plot_interval = 5    # plot the result for every 5 epoch, else might be hard to read the plot\n",
        "ep = range(num_epochs)\n",
        "plt.plot(ep[::plot_interval], my_train_loss[::plot_interval], 'r--', label=\"Training loss\")\n",
        "plt.plot(ep[::plot_interval], my_val_loss[::plot_interval], 'b--', label=\"Validation loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylim(ymax = 15, ymin = 0)\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "l6npUIdoIkd9",
        "outputId": "f503f8b0-a872-45c1-c60c-93768bebb62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f08b2a592d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training and validation losses are decreasing in general as training goes on. But we also see a lot of oscillations in the reported losses, which is usually a symptom that the learning rate may be too high or more generally the optimisation algorithm is not suitable for the current problem."
      ],
      "metadata": {
        "id": "aEl-kNqx_kYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Changing the default optimiser\n",
        "\n",
        "In the TensorFlow library, the default optimiser used is \"RMSProp\". It is possible to change the type of the optimiser within the \"compile\" function.\n",
        "\n",
        "Let us first define another neural network model which is identical to the one we used before, but we give it a different name \"MyModel2\"."
      ],
      "metadata": {
        "id": "ggi2T5OHDbUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MyModel2 = tf.keras.models.Sequential()\n",
        "MyModel2.add(tf.keras.layers.Dense(100, activation=\"relu\", input_shape=(1,)))\n",
        "for i in range(3):\n",
        "    MyModel2.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
        "MyModel2.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n",
        "\n",
        "MyModel2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Kl3ZpgV_qwM",
        "outputId": "6028e2ad-102a-4b77-95d2-1b2e149f8e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 100)               200       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,601\n",
            "Trainable params: 30,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we specify that the optimisation algorithm to be used is Adam. This can be easily done by the following line."
      ],
      "metadata": {
        "id": "ZskkbvfiEePu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MyModel2.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
      ],
      "metadata": {
        "id": "0828V1a2Eq7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we perform the model fitting and output the training and validation losses over epochs, just like what we have done before."
      ],
      "metadata": {
        "id": "i4mo2NYEEsma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MyModelResult2 = MyModel2.fit(x_train, y_train, epochs=num_epochs, batch_size=64, verbose=1, validation_data = (x_test, y_test))\n",
        "\n",
        "my_train_loss2 = MyModelResult2.history['loss']\n",
        "my_val_loss2   = MyModelResult2.history['val_loss']\n",
        "\n",
        "plot_interval = 5\n",
        "ep = range(num_epochs)\n",
        "plt.plot(ep[::plot_interval], my_train_loss2[::plot_interval], 'r--', label=\"Training loss\")\n",
        "plt.plot(ep[::plot_interval], my_val_loss2[::plot_interval], 'b--', label=\"Validation loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylim(ymax = 15, ymin = 0)\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "88rcoUyNA_gA",
        "outputId": "5d42534e-1fdc-4fae-8b96-f9d4a2722a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 1s 576ms/step - loss: 362.6845 - val_loss: 293.9992\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 355.6760 - val_loss: 287.5310\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 348.9901 - val_loss: 281.5786\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 342.8310 - val_loss: 276.0596\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 337.1198 - val_loss: 270.3284\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 331.1682 - val_loss: 264.1296\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 324.7291 - val_loss: 257.4551\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 317.7749 - val_loss: 250.1896\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 310.1949 - val_loss: 242.4790\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 302.1293 - val_loss: 234.2826\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 293.5324 - val_loss: 225.4795\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 284.2677 - val_loss: 216.0351\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 274.2868 - val_loss: 206.0250\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 263.6643 - val_loss: 195.4718\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 252.4027 - val_loss: 184.3967\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 240.5013 - val_loss: 172.9475\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 228.0947 - val_loss: 161.3369\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 215.3760 - val_loss: 149.8540\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 202.6181 - val_loss: 138.7090\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 189.9901 - val_loss: 128.2752\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 177.8396 - val_loss: 119.0255\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 166.6135 - val_loss: 111.4881\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 156.8111 - val_loss: 106.2291\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 148.9659 - val_loss: 103.8507\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 143.6821 - val_loss: 104.8457\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 141.5109 - val_loss: 109.1488\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 142.5428 - val_loss: 115.8432\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 146.1073 - val_loss: 123.3080\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 150.9024 - val_loss: 129.3037\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 155.0382 - val_loss: 132.3746\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 157.1778 - val_loss: 132.1161\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 156.8936 - val_loss: 129.2165\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 154.7246 - val_loss: 124.7539\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 151.4699 - val_loss: 119.5592\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 147.8076 - val_loss: 114.4937\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 144.4162 - val_loss: 110.0070\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 141.6505 - val_loss: 106.4041\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 139.7042 - val_loss: 103.7509\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 138.5637 - val_loss: 101.9766\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 138.1042 - val_loss: 100.9524\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 138.2012 - val_loss: 100.4168\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 138.5747 - val_loss: 100.1765\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 139.0320 - val_loss: 100.0710\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 139.4200 - val_loss: 99.9843\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 139.6344 - val_loss: 99.8468\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 139.6187 - val_loss: 99.6320\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 139.3609 - val_loss: 99.3435\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 138.8788 - val_loss: 99.0134\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 138.2197 - val_loss: 98.6888\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 137.4477 - val_loss: 98.4232\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 136.6340 - val_loss: 98.2655\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 135.8483 - val_loss: 98.2526\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 135.1503 - val_loss: 98.4008\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 134.5816 - val_loss: 98.7000\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 134.1613 - val_loss: 99.1136\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 133.8831 - val_loss: 99.5818\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 133.7171 - val_loss: 100.0288\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 133.6162 - val_loss: 100.3770\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 133.5248 - val_loss: 100.5593\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 133.3847 - val_loss: 100.5249\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 133.1535 - val_loss: 100.3297\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 132.8843 - val_loss: 99.9339\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 132.5414 - val_loss: 99.3718\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 132.1375 - val_loss: 98.6954\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 131.7021 - val_loss: 97.9619\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 131.2665 - val_loss: 97.2223\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 130.8557 - val_loss: 96.5166\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 130.4847 - val_loss: 95.8702\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 130.1571 - val_loss: 95.2953\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 129.8670 - val_loss: 94.7934\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 129.6018 - val_loss: 94.3590\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 129.3460 - val_loss: 93.9837\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 129.0849 - val_loss: 93.6591\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 128.8080 - val_loss: 93.3775\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 128.5085 - val_loss: 93.1337\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 128.1851 - val_loss: 92.9247\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 127.8411 - val_loss: 92.7485\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 127.4830 - val_loss: 92.6026\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 127.1188 - val_loss: 92.4824\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 126.7556 - val_loss: 92.3805\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 126.3985 - val_loss: 92.2862\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 126.0497 - val_loss: 92.1861\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 125.7079 - val_loss: 92.0650\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 125.3693 - val_loss: 91.9089\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 125.0285 - val_loss: 91.7064\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 124.6802 - val_loss: 91.4502\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 124.3202 - val_loss: 91.1385\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 123.9466 - val_loss: 90.7741\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 123.5594 - val_loss: 90.3640\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 123.1605 - val_loss: 89.9204\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 122.7531 - val_loss: 89.4551\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 122.3399 - val_loss: 88.9795\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 121.9227 - val_loss: 88.5037\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 121.5022 - val_loss: 88.0355\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 121.0777 - val_loss: 87.6044\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 120.7030 - val_loss: 87.2069\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 120.2178 - val_loss: 86.8551\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 119.7757 - val_loss: 86.5088\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 119.3242 - val_loss: 86.1634\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 118.8613 - val_loss: 85.8142\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 118.3874 - val_loss: 85.4575\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 117.9024 - val_loss: 85.0883\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 117.4063 - val_loss: 84.7043\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 116.9148 - val_loss: 84.3706\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 116.4049 - val_loss: 84.0544\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 115.8832 - val_loss: 83.6946\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 115.3607 - val_loss: 83.2804\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 114.8195 - val_loss: 82.8087\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 114.2569 - val_loss: 82.2808\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 113.6744 - val_loss: 81.7524\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 113.1376 - val_loss: 81.3133\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 112.5005 - val_loss: 80.8706\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 111.9117 - val_loss: 80.3681\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 111.3110 - val_loss: 79.8166\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 110.6661 - val_loss: 79.2195\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 110.0233 - val_loss: 78.6410\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 109.3458 - val_loss: 78.0853\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 108.6679 - val_loss: 77.4924\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 108.0241 - val_loss: 76.9174\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 107.2797 - val_loss: 76.3653\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 106.5569 - val_loss: 75.7440\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 105.8101 - val_loss: 74.9932\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 104.9636 - val_loss: 74.2246\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 104.2069 - val_loss: 73.5155\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 103.3674 - val_loss: 72.9005\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 102.5066 - val_loss: 72.2470\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 101.6576 - val_loss: 71.5219\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 100.7688 - val_loss: 70.7765\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 99.8843 - val_loss: 70.0636\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 98.9231 - val_loss: 69.3160\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 97.9668 - val_loss: 68.4989\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 96.9908 - val_loss: 67.6115\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 95.9304 - val_loss: 66.7195\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 94.9445 - val_loss: 65.8683\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 93.8050 - val_loss: 65.1544\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 92.6425 - val_loss: 64.3466\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 91.5631 - val_loss: 63.4673\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 90.4684 - val_loss: 62.5633\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 89.3112 - val_loss: 61.5148\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 88.0813 - val_loss: 60.3940\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 86.8122 - val_loss: 59.3498\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 85.5895 - val_loss: 58.4747\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 84.3651 - val_loss: 57.6027\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 83.0254 - val_loss: 56.6754\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 81.6783 - val_loss: 55.5899\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 80.3629 - val_loss: 54.4868\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 78.9954 - val_loss: 53.4416\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 77.5703 - val_loss: 52.4042\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 76.1569 - val_loss: 51.2937\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 74.6902 - val_loss: 50.0432\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 73.2047 - val_loss: 48.7792\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 71.6809 - val_loss: 47.5934\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 70.1853 - val_loss: 46.5513\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 68.6348 - val_loss: 45.4625\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 67.0441 - val_loss: 44.3036\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 65.4208 - val_loss: 43.0233\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 63.8818 - val_loss: 41.9673\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 62.2312 - val_loss: 41.2071\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 60.6048 - val_loss: 40.1278\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 58.9630 - val_loss: 38.7498\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 57.2303 - val_loss: 37.2770\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 55.4935 - val_loss: 36.0980\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 53.8486 - val_loss: 35.1512\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 52.1400 - val_loss: 34.1225\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 50.3857 - val_loss: 32.8110\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 48.5491 - val_loss: 31.3211\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 46.8040 - val_loss: 30.1661\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 45.0073 - val_loss: 29.1074\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 43.2346 - val_loss: 27.8969\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 41.4403 - val_loss: 26.5437\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 39.5643 - val_loss: 25.2422\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 37.6957 - val_loss: 24.0123\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 35.8571 - val_loss: 22.8497\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 34.0298 - val_loss: 21.5562\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 32.1823 - val_loss: 20.2865\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 30.2810 - val_loss: 18.9066\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 28.3622 - val_loss: 17.5897\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 26.4905 - val_loss: 16.3548\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 24.5833 - val_loss: 15.2251\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 22.8200 - val_loss: 13.9694\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 21.0406 - val_loss: 12.7853\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 19.3404 - val_loss: 11.7295\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 17.7292 - val_loss: 10.7272\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 16.1471 - val_loss: 9.7082\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 14.5954 - val_loss: 8.7628\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 13.1401 - val_loss: 7.9362\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 11.7848 - val_loss: 7.1995\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 10.5627 - val_loss: 6.5930\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 9.4987 - val_loss: 6.1425\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 8.5991 - val_loss: 5.7162\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 7.7717 - val_loss: 5.3563\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 7.0455 - val_loss: 5.0539\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 6.4073 - val_loss: 4.8635\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 5.8557 - val_loss: 4.6278\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 5.3545 - val_loss: 4.3875\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 4.9088 - val_loss: 4.1619\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 4.4772 - val_loss: 3.8758\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 4.0840 - val_loss: 3.5639\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.7104 - val_loss: 3.3077\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3.3769 - val_loss: 3.0511\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 3.0965 - val_loss: 2.7973\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 2.8515 - val_loss: 2.6417\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.6351 - val_loss: 2.5064\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 2.4344 - val_loss: 2.3663\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 2.2373 - val_loss: 2.3102\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.0423 - val_loss: 2.2268\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.8567 - val_loss: 2.1968\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.7132 - val_loss: 2.2581\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.6388 - val_loss: 2.3034\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.5970 - val_loss: 2.3946\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.5723 - val_loss: 2.4606\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.5594 - val_loss: 2.4469\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.5458 - val_loss: 2.4548\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.5362 - val_loss: 2.4756\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.5263 - val_loss: 2.4789\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.5081 - val_loss: 2.5008\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.4921 - val_loss: 2.4564\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.4827 - val_loss: 2.3892\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.4778 - val_loss: 2.3611\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.4738 - val_loss: 2.3242\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.4634 - val_loss: 2.3296\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 1.4493 - val_loss: 2.3053\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.4311 - val_loss: 2.2729\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.4095 - val_loss: 2.2469\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.3852 - val_loss: 2.2286\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3609 - val_loss: 2.2171\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.3357 - val_loss: 2.1976\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3108 - val_loss: 2.1684\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.2856 - val_loss: 2.1428\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.2643 - val_loss: 2.1145\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2470 - val_loss: 2.0827\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.2340 - val_loss: 2.0627\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2214 - val_loss: 2.0438\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.2143 - val_loss: 2.0239\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.2085 - val_loss: 2.0006\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2032 - val_loss: 1.9746\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.1991 - val_loss: 1.9558\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.1883 - val_loss: 1.9532\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.1716 - val_loss: 1.9544\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.1538 - val_loss: 1.9499\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.1381 - val_loss: 1.9418\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1259 - val_loss: 1.9434\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1143 - val_loss: 1.9528\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.1011 - val_loss: 1.9635\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0930 - val_loss: 1.9586\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0836 - val_loss: 1.9417\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0736 - val_loss: 1.9212\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.0664 - val_loss: 1.9080\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.0590 - val_loss: 1.9033\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.0541 - val_loss: 1.8966\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.0518 - val_loss: 1.8905\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.0480 - val_loss: 1.8859\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.0419 - val_loss: 1.8857\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.0342 - val_loss: 1.8908\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.0282 - val_loss: 1.8890\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.0216 - val_loss: 1.8813\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 1.0156 - val_loss: 1.8753\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0105 - val_loss: 1.8823\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0052 - val_loss: 1.8903\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 1.0003 - val_loss: 1.9035\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.9962 - val_loss: 1.9042\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.9922 - val_loss: 1.9133\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.9885 - val_loss: 1.8999\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.9865 - val_loss: 1.9181\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.9895 - val_loss: 1.8978\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.0010 - val_loss: 1.9521\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0208 - val_loss: 1.9016\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.0364 - val_loss: 1.9456\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.0307 - val_loss: 1.8428\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.9912 - val_loss: 1.8271\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.9571 - val_loss: 1.8248\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.9553 - val_loss: 1.8185\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.9737 - val_loss: 1.8791\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9808 - val_loss: 1.8181\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9600 - val_loss: 1.8197\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9400 - val_loss: 1.8234\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9403 - val_loss: 1.8022\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.9501 - val_loss: 1.8387\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.9501 - val_loss: 1.7869\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9347 - val_loss: 1.7937\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.9236 - val_loss: 1.8162\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.9257 - val_loss: 1.8009\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.9309 - val_loss: 1.8378\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.9270 - val_loss: 1.7980\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.9160 - val_loss: 1.7971\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.9100 - val_loss: 1.8111\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.9123 - val_loss: 1.7885\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.9145 - val_loss: 1.8214\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.9099 - val_loss: 1.7968\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.9024 - val_loss: 1.8001\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.8982 - val_loss: 1.8087\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.8990 - val_loss: 1.7817\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.8995 - val_loss: 1.8051\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8982 - val_loss: 1.7742\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.8928 - val_loss: 1.7829\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.8880 - val_loss: 1.7854\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.8863 - val_loss: 1.7709\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8867 - val_loss: 1.7956\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.8868 - val_loss: 1.7683\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.8837 - val_loss: 1.7818\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.8793 - val_loss: 1.7676\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.8714 - val_loss: 1.7645\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8632 - val_loss: 1.7887\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.8551 - val_loss: 1.7733\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8470 - val_loss: 1.8011\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.8398 - val_loss: 1.7699\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.8361 - val_loss: 1.7728\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.8322 - val_loss: 1.7330\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8282 - val_loss: 1.7120\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.8253 - val_loss: 1.6874\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8234 - val_loss: 1.6582\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.8223 - val_loss: 1.6550\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.8217 - val_loss: 1.6284\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.8212 - val_loss: 1.6469\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.8198 - val_loss: 1.6258\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.8174 - val_loss: 1.6526\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.8140 - val_loss: 1.6371\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8103 - val_loss: 1.6682\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.8075 - val_loss: 1.6614\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.8049 - val_loss: 1.6934\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.8027 - val_loss: 1.6873\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8007 - val_loss: 1.7127\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7989 - val_loss: 1.7014\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7971 - val_loss: 1.7184\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7957 - val_loss: 1.6990\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7946 - val_loss: 1.7173\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7940 - val_loss: 1.6841\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7936 - val_loss: 1.7173\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7951 - val_loss: 1.6703\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.7963 - val_loss: 1.7302\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7999 - val_loss: 1.6655\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.8025 - val_loss: 1.7486\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8075 - val_loss: 1.6624\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.8085 - val_loss: 1.7562\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.8105 - val_loss: 1.6602\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.8051 - val_loss: 1.7372\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7990 - val_loss: 1.6553\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7883 - val_loss: 1.6981\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7806 - val_loss: 1.6614\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7744 - val_loss: 1.6678\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7720 - val_loss: 1.6858\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7732 - val_loss: 1.6521\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7754 - val_loss: 1.7011\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7786 - val_loss: 1.6408\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7793 - val_loss: 1.6981\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7797 - val_loss: 1.6274\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7770 - val_loss: 1.6785\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7749 - val_loss: 1.6238\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7707 - val_loss: 1.6611\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7662 - val_loss: 1.6387\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7628 - val_loss: 1.6468\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7613 - val_loss: 1.6630\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7616 - val_loss: 1.6441\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7633 - val_loss: 1.6953\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7662 - val_loss: 1.6487\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7694 - val_loss: 1.7250\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7742 - val_loss: 1.6466\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7770 - val_loss: 1.7349\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7809 - val_loss: 1.6357\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7800 - val_loss: 1.7242\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7786 - val_loss: 1.6294\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7715 - val_loss: 1.6945\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7646 - val_loss: 1.6371\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7572 - val_loss: 1.6663\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7528 - val_loss: 1.6605\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.7513 - val_loss: 1.6499\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7518 - val_loss: 1.6799\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7540 - val_loss: 1.6386\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7565 - val_loss: 1.6973\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7595 - val_loss: 1.6364\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7612 - val_loss: 1.7079\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7623 - val_loss: 1.6369\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7611 - val_loss: 1.7039\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7593 - val_loss: 1.6374\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7563 - val_loss: 1.6874\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7530 - val_loss: 1.6377\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7493 - val_loss: 1.6653\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.7466 - val_loss: 1.6475\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7448 - val_loss: 1.6500\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7441 - val_loss: 1.6595\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7442 - val_loss: 1.6405\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7449 - val_loss: 1.6749\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7464 - val_loss: 1.6381\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7476 - val_loss: 1.6931\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7492 - val_loss: 1.6390\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7506 - val_loss: 1.7024\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7524 - val_loss: 1.6347\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7536 - val_loss: 1.7102\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7561 - val_loss: 1.6323\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7581 - val_loss: 1.7225\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7609 - val_loss: 1.6329\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7620 - val_loss: 1.7308\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7628 - val_loss: 1.6353\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7606 - val_loss: 1.7238\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7572 - val_loss: 1.6381\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7507 - val_loss: 1.6953\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7448 - val_loss: 1.6451\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.7400 - val_loss: 1.6664\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7372 - val_loss: 1.6624\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7363 - val_loss: 1.6516\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7370 - val_loss: 1.6785\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7384 - val_loss: 1.6402\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7404 - val_loss: 1.6905\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.7429 - val_loss: 1.6306\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7453 - val_loss: 1.7010\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7476 - val_loss: 1.6291\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7500 - val_loss: 1.7161\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7520 - val_loss: 1.6336\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7528 - val_loss: 1.7247\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7527 - val_loss: 1.6370\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.7512 - val_loss: 1.7191\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7484 - val_loss: 1.6377\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7436 - val_loss: 1.6933\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7388 - val_loss: 1.6453\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7349 - val_loss: 1.6712\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7325 - val_loss: 1.6580\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7314 - val_loss: 1.6559\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7313 - val_loss: 1.6727\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7319 - val_loss: 1.6462\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7329 - val_loss: 1.6838\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7343 - val_loss: 1.6387\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7373 - val_loss: 1.7039\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7410 - val_loss: 1.6339\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7460 - val_loss: 1.7304\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7518 - val_loss: 1.6307\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7560 - val_loss: 1.7447\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7598 - val_loss: 1.6280\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7596 - val_loss: 1.7400\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7570 - val_loss: 1.6278\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7490 - val_loss: 1.7060\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7400 - val_loss: 1.6413\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7325 - val_loss: 1.6703\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7282 - val_loss: 1.6652\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.7275 - val_loss: 1.6465\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7293 - val_loss: 1.6899\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7323 - val_loss: 1.6364\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7350 - val_loss: 1.7031\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7371 - val_loss: 1.6345\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7375 - val_loss: 1.7049\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7369 - val_loss: 1.6362\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7363 - val_loss: 1.7023\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7354 - val_loss: 1.6380\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.7344 - val_loss: 1.6994\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7335 - val_loss: 1.6390\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.7329 - val_loss: 1.6940\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7323 - val_loss: 1.6330\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7324 - val_loss: 1.6907\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7321 - val_loss: 1.6321\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7311 - val_loss: 1.6862\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7299 - val_loss: 1.6370\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7288 - val_loss: 1.6808\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7275 - val_loss: 1.6404\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7263 - val_loss: 1.6747\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7254 - val_loss: 1.6448\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7245 - val_loss: 1.6698\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7238 - val_loss: 1.6501\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7232 - val_loss: 1.6668\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7227 - val_loss: 1.6524\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7227 - val_loss: 1.6722\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7231 - val_loss: 1.6469\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7240 - val_loss: 1.6872\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7264 - val_loss: 1.6362\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7309 - val_loss: 1.7206\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7390 - val_loss: 1.6316\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7512 - val_loss: 1.7808\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7682 - val_loss: 1.6415\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7834 - val_loss: 1.8256\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7930 - val_loss: 1.6391\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7882 - val_loss: 1.7817\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.7695 - val_loss: 1.6262\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7458 - val_loss: 1.6908\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7274 - val_loss: 1.6541\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7202 - val_loss: 1.6410\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7242 - val_loss: 1.7181\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7346 - val_loss: 1.6348\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7443 - val_loss: 1.7511\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7484 - val_loss: 1.6334\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7436 - val_loss: 1.7148\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7342 - val_loss: 1.6356\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7247 - val_loss: 1.6636\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7194 - val_loss: 1.6631\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.7191 - val_loss: 1.6406\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7225 - val_loss: 1.7018\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7274 - val_loss: 1.6377\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7328 - val_loss: 1.7265\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7362 - val_loss: 1.6377\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7363 - val_loss: 1.7204\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7336 - val_loss: 1.6374\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7290 - val_loss: 1.6927\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7241 - val_loss: 1.6426\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7205 - val_loss: 1.6680\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7181 - val_loss: 1.6564\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7171 - val_loss: 1.6496\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7175 - val_loss: 1.6740\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7187 - val_loss: 1.6393\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7220 - val_loss: 1.7002\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7257 - val_loss: 1.6359\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7302 - val_loss: 1.7233\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7335 - val_loss: 1.6347\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7355 - val_loss: 1.7257\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7349 - val_loss: 1.6340\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7328 - val_loss: 1.7121\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7288 - val_loss: 1.6374\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7236 - val_loss: 1.6774\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7189 - val_loss: 1.6482\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7163 - val_loss: 1.6515\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.7159 - val_loss: 1.6710\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7173 - val_loss: 1.6397\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7195 - val_loss: 1.6920\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7214 - val_loss: 1.6397\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7226 - val_loss: 1.6988\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7228 - val_loss: 1.6402\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7233 - val_loss: 1.7008\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7233 - val_loss: 1.6387\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7232 - val_loss: 1.6983\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7232 - val_loss: 1.6377\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7229 - val_loss: 1.6986\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7223 - val_loss: 1.6400\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7220 - val_loss: 1.6977\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7215 - val_loss: 1.6398\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7218 - val_loss: 1.6973\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7213 - val_loss: 1.6374\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7223 - val_loss: 1.7003\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7220 - val_loss: 1.6393\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7210 - val_loss: 1.6926\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7194 - val_loss: 1.6435\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7175 - val_loss: 1.6783\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7158 - val_loss: 1.6502\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7144 - val_loss: 1.6646\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7137 - val_loss: 1.6602\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7133 - val_loss: 1.6560\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7132 - val_loss: 1.6660\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7134 - val_loss: 1.6487\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7137 - val_loss: 1.6714\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7146 - val_loss: 1.6417\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7160 - val_loss: 1.6914\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7198 - val_loss: 1.6360\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7245 - val_loss: 1.7292\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7333 - val_loss: 1.6359\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7430 - val_loss: 1.7762\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7561 - val_loss: 1.6383\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7656 - val_loss: 1.8010\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.7712 - val_loss: 1.6349\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7662 - val_loss: 1.7689\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7532 - val_loss: 1.6296\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7354 - val_loss: 1.7042\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7216 - val_loss: 1.6425\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7130 - val_loss: 1.6533\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7099 - val_loss: 1.6688\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7123 - val_loss: 1.6279\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7195 - val_loss: 1.7197\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7290 - val_loss: 1.6302\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7382 - val_loss: 1.7587\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7421 - val_loss: 1.6436\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7415 - val_loss: 1.7551\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7339 - val_loss: 1.6483\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7269 - val_loss: 1.7069\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7171 - val_loss: 1.6489\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7114 - val_loss: 1.6493\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7104 - val_loss: 1.6752\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7129 - val_loss: 1.6385\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7171 - val_loss: 1.7157\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7210 - val_loss: 1.6457\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7224 - val_loss: 1.7222\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7212 - val_loss: 1.6454\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7213 - val_loss: 1.7069\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7184 - val_loss: 1.6336\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.7164 - val_loss: 1.6803\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7141 - val_loss: 1.6329\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7115 - val_loss: 1.6606\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7102 - val_loss: 1.6510\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7090 - val_loss: 1.6483\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7094 - val_loss: 1.6729\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7100 - val_loss: 1.6509\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7104 - val_loss: 1.6902\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7122 - val_loss: 1.6442\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7128 - val_loss: 1.6941\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7161 - val_loss: 1.6298\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7189 - val_loss: 1.7046\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7221 - val_loss: 1.6305\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7233 - val_loss: 1.7198\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7250 - val_loss: 1.6414\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7242 - val_loss: 1.7309\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7239 - val_loss: 1.6486\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7231 - val_loss: 1.7225\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7212 - val_loss: 1.6354\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7192 - val_loss: 1.6947\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7177 - val_loss: 1.6263\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7168 - val_loss: 1.6876\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7148 - val_loss: 1.6374\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7144 - val_loss: 1.6981\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7127 - val_loss: 1.6525\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.7117 - val_loss: 1.6966\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7096 - val_loss: 1.6579\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7081 - val_loss: 1.6755\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7079 - val_loss: 1.6528\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7060 - val_loss: 1.6534\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7062 - val_loss: 1.6597\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7060 - val_loss: 1.6487\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7063 - val_loss: 1.6771\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7077 - val_loss: 1.6488\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7099 - val_loss: 1.7021\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7134 - val_loss: 1.6383\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7196 - val_loss: 1.7332\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7279 - val_loss: 1.6322\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7378 - val_loss: 1.7717\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7483 - val_loss: 1.6375\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7557 - val_loss: 1.7970\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7581 - val_loss: 1.6439\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.7542 - val_loss: 1.7682\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7426 - val_loss: 1.6314\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7296 - val_loss: 1.7082\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7181 - val_loss: 1.6418\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7107 - val_loss: 1.6792\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7064 - val_loss: 1.6746\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7050 - val_loss: 1.6627\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7061 - val_loss: 1.7033\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7103 - val_loss: 1.6469\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7161 - val_loss: 1.7270\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.7229 - val_loss: 1.6303\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7290 - val_loss: 1.7414\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7327 - val_loss: 1.6347\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7322 - val_loss: 1.7454\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7274 - val_loss: 1.6478\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7208 - val_loss: 1.7221\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7163 - val_loss: 1.6576\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7087 - val_loss: 1.6790\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7059 - val_loss: 1.6640\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.7043 - val_loss: 1.6419\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7076 - val_loss: 1.6855\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7090 - val_loss: 1.6408\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7124 - val_loss: 1.7088\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7137 - val_loss: 1.6395\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7169 - val_loss: 1.7237\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7195 - val_loss: 1.6393\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7216 - val_loss: 1.7290\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7221 - val_loss: 1.6421\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7212 - val_loss: 1.7306\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7194 - val_loss: 1.6490\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7169 - val_loss: 1.7227\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.7163 - val_loss: 1.6477\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7136 - val_loss: 1.7013\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7099 - val_loss: 1.6383\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7088 - val_loss: 1.6859\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7080 - val_loss: 1.6453\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7057 - val_loss: 1.6842\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7040 - val_loss: 1.6652\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7032 - val_loss: 1.6772\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7022 - val_loss: 1.6742\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7025 - val_loss: 1.6613\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7031 - val_loss: 1.6786\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7037 - val_loss: 1.6514\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7059 - val_loss: 1.7067\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7097 - val_loss: 1.6458\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7154 - val_loss: 1.7389\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7232 - val_loss: 1.6382\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7311 - val_loss: 1.7604\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7373 - val_loss: 1.6329\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7397 - val_loss: 1.7585\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.7368 - val_loss: 1.6371\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7305 - val_loss: 1.7394\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7215 - val_loss: 1.6495\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7155 - val_loss: 1.7123\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7090 - val_loss: 1.6522\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7058 - val_loss: 1.6846\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7038 - val_loss: 1.6588\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7023 - val_loss: 1.6690\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7013 - val_loss: 1.6796\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7013 - val_loss: 1.6609\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7029 - val_loss: 1.6921\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7037 - val_loss: 1.6494\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7048 - val_loss: 1.6888\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7056 - val_loss: 1.6385\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.7087 - val_loss: 1.7076\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7113 - val_loss: 1.6434\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7163 - val_loss: 1.7417\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7222 - val_loss: 1.6434\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7269 - val_loss: 1.7546\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7296 - val_loss: 1.6381\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7305 - val_loss: 1.7481\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7284 - val_loss: 1.6394\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7248 - val_loss: 1.7362\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7187 - val_loss: 1.6477\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7139 - val_loss: 1.7141\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7088 - val_loss: 1.6461\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7069 - val_loss: 1.6917\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7048 - val_loss: 1.6470\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7027 - val_loss: 1.6759\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7009 - val_loss: 1.6634\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6996 - val_loss: 1.6709\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7000 - val_loss: 1.6808\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7000 - val_loss: 1.6599\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7006 - val_loss: 1.6831\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7015 - val_loss: 1.6471\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7026 - val_loss: 1.6881\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7035 - val_loss: 1.6460\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7062 - val_loss: 1.7177\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7112 - val_loss: 1.6438\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7168 - val_loss: 1.7457\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7246 - val_loss: 1.6397\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7313 - val_loss: 1.7684\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7379 - val_loss: 1.6433\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.7402 - val_loss: 1.7789\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7391 - val_loss: 1.6454\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.7340 - val_loss: 1.7476\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7244 - val_loss: 1.6380\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.7143 - val_loss: 1.6992\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7071 - val_loss: 1.6433\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7029 - val_loss: 1.6829\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7000 - val_loss: 1.6585\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6993 - val_loss: 1.6883\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7000 - val_loss: 1.6637\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6992 - val_loss: 1.6782\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6984 - val_loss: 1.6612\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6980 - val_loss: 1.6666\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6981 - val_loss: 1.6653\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6975 - val_loss: 1.6658\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6972 - val_loss: 1.6728\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6983 - val_loss: 1.6627\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6980 - val_loss: 1.6706\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6979 - val_loss: 1.6480\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6993 - val_loss: 1.6915\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7030 - val_loss: 1.6452\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7085 - val_loss: 1.7425\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.7179 - val_loss: 1.6483\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7288 - val_loss: 1.7813\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7390 - val_loss: 1.6439\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7464 - val_loss: 1.7916\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7488 - val_loss: 1.6372\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7443 - val_loss: 1.7595\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7332 - val_loss: 1.6349\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7221 - val_loss: 1.7192\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7107 - val_loss: 1.6471\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7027 - val_loss: 1.6830\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6986 - val_loss: 1.6688\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6968 - val_loss: 1.6704\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6966 - val_loss: 1.6983\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.6982 - val_loss: 1.6531\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.7047 - val_loss: 1.7275\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7111 - val_loss: 1.6368\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7218 - val_loss: 1.7539\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7290 - val_loss: 1.6348\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7335 - val_loss: 1.7651\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7302 - val_loss: 1.6468\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7241 - val_loss: 1.7450\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7133 - val_loss: 1.6582\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7065 - val_loss: 1.7086\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7000 - val_loss: 1.6644\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.6960 - val_loss: 1.6582\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6947 - val_loss: 1.6699\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6974 - val_loss: 1.6357\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7008 - val_loss: 1.7033\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7041 - val_loss: 1.6439\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7102 - val_loss: 1.7466\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7150 - val_loss: 1.6559\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7212 - val_loss: 1.7638\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7218 - val_loss: 1.6424\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7207 - val_loss: 1.7287\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7159 - val_loss: 1.6297\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7112 - val_loss: 1.7036\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.7045 - val_loss: 1.6449\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7013 - val_loss: 1.6966\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6978 - val_loss: 1.6632\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6961 - val_loss: 1.6873\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6945 - val_loss: 1.6772\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6944 - val_loss: 1.6663\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6945 - val_loss: 1.6806\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6955 - val_loss: 1.6606\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6954 - val_loss: 1.6975\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6964 - val_loss: 1.6553\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7009 - val_loss: 1.7162\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7046 - val_loss: 1.6431\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7102 - val_loss: 1.7409\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.7174 - val_loss: 1.6377\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7251 - val_loss: 1.7600\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7266 - val_loss: 1.6443\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7281 - val_loss: 1.7549\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7231 - val_loss: 1.6456\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7189 - val_loss: 1.7380\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7108 - val_loss: 1.6558\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7070 - val_loss: 1.7143\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7025 - val_loss: 1.6456\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6997 - val_loss: 1.6869\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6982 - val_loss: 1.6409\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6971 - val_loss: 1.6709\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6953 - val_loss: 1.6603\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6934 - val_loss: 1.6752\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6922 - val_loss: 1.6853\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6924 - val_loss: 1.6761\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6944 - val_loss: 1.6875\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6937 - val_loss: 1.6470\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6952 - val_loss: 1.6968\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7016 - val_loss: 1.6183\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7104 - val_loss: 1.7413\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7186 - val_loss: 1.6607\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7283 - val_loss: 1.8011\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7346 - val_loss: 1.6669\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7341 - val_loss: 1.7877\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7314 - val_loss: 1.6403\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7230 - val_loss: 1.7262\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7148 - val_loss: 1.6319\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7057 - val_loss: 1.7003\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6997 - val_loss: 1.6519\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6978 - val_loss: 1.6918\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6966 - val_loss: 1.6637\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6934 - val_loss: 1.6800\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6922 - val_loss: 1.6708\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6915 - val_loss: 1.6660\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6929 - val_loss: 1.6843\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6928 - val_loss: 1.6671\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6936 - val_loss: 1.6969\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6935 - val_loss: 1.6563\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6977 - val_loss: 1.7205\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7015 - val_loss: 1.6482\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7098 - val_loss: 1.7511\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7170 - val_loss: 1.6400\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7236 - val_loss: 1.7646\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7255 - val_loss: 1.6452\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7249 - val_loss: 1.7618\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7204 - val_loss: 1.6455\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7142 - val_loss: 1.7291\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7051 - val_loss: 1.6477\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7013 - val_loss: 1.7046\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6970 - val_loss: 1.6556\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6963 - val_loss: 1.7043\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6938 - val_loss: 1.6680\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6923 - val_loss: 1.6941\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6908 - val_loss: 1.6642\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6905 - val_loss: 1.6711\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.6905 - val_loss: 1.6609\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6896 - val_loss: 1.6643\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6895 - val_loss: 1.6724\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6891 - val_loss: 1.6717\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6911 - val_loss: 1.6894\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6912 - val_loss: 1.6682\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6917 - val_loss: 1.7075\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6954 - val_loss: 1.6500\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7026 - val_loss: 1.7473\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7118 - val_loss: 1.6489\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7250 - val_loss: 1.7909\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.7372 - val_loss: 1.6457\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7448 - val_loss: 1.8066\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7479 - val_loss: 1.6486\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7446 - val_loss: 1.7863\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7313 - val_loss: 1.6530\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7179 - val_loss: 1.7387\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7026 - val_loss: 1.6671\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6944 - val_loss: 1.6975\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6887 - val_loss: 1.6808\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6893 - val_loss: 1.6632\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6898 - val_loss: 1.6924\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6920 - val_loss: 1.6387\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6990 - val_loss: 1.7291\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7072 - val_loss: 1.6446\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7174 - val_loss: 1.7770\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7220 - val_loss: 1.6598\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7251 - val_loss: 1.7770\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7179 - val_loss: 1.6573\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7104 - val_loss: 1.7289\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.7011 - val_loss: 1.6489\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6968 - val_loss: 1.6935\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6910 - val_loss: 1.6621\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6883 - val_loss: 1.6711\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6883 - val_loss: 1.6837\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6890 - val_loss: 1.6623\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6901 - val_loss: 1.7071\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6917 - val_loss: 1.6560\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6983 - val_loss: 1.7438\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7038 - val_loss: 1.6583\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7136 - val_loss: 1.7692\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7174 - val_loss: 1.6501\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7208 - val_loss: 1.7624\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7156 - val_loss: 1.6515\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7121 - val_loss: 1.7372\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.7031 - val_loss: 1.6525\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6983 - val_loss: 1.7117\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6921 - val_loss: 1.6644\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6906 - val_loss: 1.7066\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6896 - val_loss: 1.6666\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6878 - val_loss: 1.6845\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6863 - val_loss: 1.6636\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6863 - val_loss: 1.6736\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6859 - val_loss: 1.6725\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6853 - val_loss: 1.6747\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6858 - val_loss: 1.6885\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6857 - val_loss: 1.6783\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6868 - val_loss: 1.6920\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6860 - val_loss: 1.6642\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6908 - val_loss: 1.7139\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6929 - val_loss: 1.6535\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7010 - val_loss: 1.7546\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.7108 - val_loss: 1.6506\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.7220 - val_loss: 1.7917\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7301 - val_loss: 1.6560\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7323 - val_loss: 1.7938\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.7293 - val_loss: 1.6584\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7205 - val_loss: 1.7616\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7100 - val_loss: 1.6557\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7009 - val_loss: 1.7156\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6936 - val_loss: 1.6626\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6885 - val_loss: 1.6972\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6869 - val_loss: 1.6666\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6880 - val_loss: 1.7028\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6887 - val_loss: 1.6701\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6879 - val_loss: 1.7066\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6885 - val_loss: 1.6658\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6882 - val_loss: 1.7020\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6888 - val_loss: 1.6706\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6881 - val_loss: 1.7054\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6879 - val_loss: 1.6627\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6887 - val_loss: 1.7179\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6927 - val_loss: 1.6533\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.7005 - val_loss: 1.7536\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.7083 - val_loss: 1.6542\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7174 - val_loss: 1.7892\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.7219 - val_loss: 1.6662\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7263 - val_loss: 1.7955\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7229 - val_loss: 1.6545\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7154 - val_loss: 1.7483\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7060 - val_loss: 1.6496\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.6992 - val_loss: 1.7202\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6919 - val_loss: 1.6663\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6901 - val_loss: 1.7168\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6875 - val_loss: 1.6785\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6850 - val_loss: 1.6990\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6848 - val_loss: 1.6749\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6842 - val_loss: 1.6797\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6829 - val_loss: 1.6785\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6826 - val_loss: 1.6687\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6831 - val_loss: 1.6899\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6843 - val_loss: 1.6732\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6847 - val_loss: 1.7140\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6857 - val_loss: 1.6783\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6892 - val_loss: 1.7512\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6960 - val_loss: 1.6672\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.7042 - val_loss: 1.7724\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7145 - val_loss: 1.6556\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7230 - val_loss: 1.7960\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7271 - val_loss: 1.6633\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7267 - val_loss: 1.7948\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7199 - val_loss: 1.6693\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7136 - val_loss: 1.7629\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7035 - val_loss: 1.6686\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.6948 - val_loss: 1.7212\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6884 - val_loss: 1.6652\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6869 - val_loss: 1.7067\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6841 - val_loss: 1.6736\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6860 - val_loss: 1.7192\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6852 - val_loss: 1.6721\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6925 - val_loss: 1.7466\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6948 - val_loss: 1.6630\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7025 - val_loss: 1.7644\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7060 - val_loss: 1.6576\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7147 - val_loss: 1.7816\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7120 - val_loss: 1.6657\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.7121 - val_loss: 1.7725\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7040 - val_loss: 1.6692\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7014 - val_loss: 1.7514\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6930 - val_loss: 1.6833\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6871 - val_loss: 1.7148\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6822 - val_loss: 1.6988\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6814 - val_loss: 1.6745\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6827 - val_loss: 1.7187\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6870 - val_loss: 1.6651\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6898 - val_loss: 1.7400\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6918 - val_loss: 1.6754\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6911 - val_loss: 1.7470\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6893 - val_loss: 1.6791\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6927 - val_loss: 1.7534\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6926 - val_loss: 1.6712\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6974 - val_loss: 1.7549\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6986 - val_loss: 1.6637\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7036 - val_loss: 1.7687\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.7039 - val_loss: 1.6713\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.7069 - val_loss: 1.7801\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7066 - val_loss: 1.6708\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7058 - val_loss: 1.7654\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6999 - val_loss: 1.6657\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6977 - val_loss: 1.7441\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6933 - val_loss: 1.6711\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6926 - val_loss: 1.7449\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6891 - val_loss: 1.6862\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6852 - val_loss: 1.7282\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6831 - val_loss: 1.6948\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6809 - val_loss: 1.7000\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6791 - val_loss: 1.6946\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6788 - val_loss: 1.6827\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6799 - val_loss: 1.7166\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6815 - val_loss: 1.6904\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6823 - val_loss: 1.7440\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.6845 - val_loss: 1.6968\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6900 - val_loss: 1.7812\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6978 - val_loss: 1.6835\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7049 - val_loss: 1.7969\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7149 - val_loss: 1.6721\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.7196 - val_loss: 1.8103\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7244 - val_loss: 1.6690\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.7201 - val_loss: 1.7907\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7134 - val_loss: 1.6773\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.7033 - val_loss: 1.7662\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f08b4404e90>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we switch to Adam, the losses over epoch are now \"smoother\". The main reason is that the Adam algorithm has a momentum feature in the update rule with the gradient descent equation. The direction of update tends to be more stable over successive iterations and therefore there are less oscillations."
      ],
      "metadata": {
        "id": "QGO8q48TFps-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remark: possibility of overfitting\n",
        "\n",
        "One thing to notice is that the validation loss is consistently higher than the training loss when number of epochs performed is above around 200. Beyond this point, further training does not help the model explain the data in the validation set (i.e. there is a risk of overfitting if we go beyond 200 epochs for this particular model and data set)."
      ],
      "metadata": {
        "id": "cXSMxZDL9QRd"
      }
    }
  ]
}